{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import datasets\n",
    "ds = datasets.load_from_disk('../data/hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "for i in range(0, 8):\n",
    "    batch.append(ds['train'][i])\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRAlphabet:\n",
    "    def __init__(self, alphabet_string):\n",
    "        self.alphabet = list(alphabet_string)\n",
    "        self.alphabet.append('<blank>')  # Adding blank token\n",
    "        self.char_to_index = {char: index for index, char in enumerate(self.alphabet)}\n",
    "        self.index_to_char = {index: char for index, char in enumerate(self.alphabet)}\n",
    "\n",
    "    def text_to_array(self, text):\n",
    "        return [self.char_to_index[char] for char in str.lower(text) if char in self.char_to_index]\n",
    "\n",
    "    def array_to_text(self, array):\n",
    "        return ''.join(self.index_to_char[index] for index in array if index in self.index_to_char)\n",
    "    \n",
    "alphabet = ASRAlphabet(alphabet_string='abcdefghijklmnopqrstuvwxyzåäö ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test alphabet\n",
    "\n",
    "sample_s = batch[0]['sentence']\n",
    "\n",
    "aout1 = alphabet.text_to_array(sample_s)\n",
    "aout2 = alphabet.array_to_text(aout1)\n",
    "\n",
    "print(sample_s, aout1, aout2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "mel_spectrogram_converter = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_fft=400,\n",
    "        n_mels=40\n",
    ")\n",
    "\n",
    "# from audio_utils\n",
    "def preprocess_audio(batch):\n",
    "\n",
    "    max_input_length = 0\n",
    "    max_label_length = 0\n",
    "\n",
    "    audio = []\n",
    "    label = []\n",
    "    audio_length = []\n",
    "    label_length = []\n",
    "\n",
    "    for item in batch:\n",
    "        t1 = torch.tensor(item['audio']['array']).float()\n",
    "\n",
    "        sample_rate = item['audio']['sampling_rate']\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            t1 = resampler(t1)\n",
    "        \n",
    "        t1 = torchaudio.functional.preemphasis(t1)\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram_converter(t1)\n",
    "\n",
    "        # Apply log to the mel spectrogram\n",
    "        mel_spectrogram = torch.log(mel_spectrogram + 1e-9)\n",
    "\n",
    "        # Normalize the spectrogram\n",
    "        mel_spectrogram = (mel_spectrogram - mel_spectrogram.mean()) / mel_spectrogram.std()\n",
    "\n",
    "        # Transpose the mel spectrogram to correct dimension (time, mels)\n",
    "        mel_spectrogram = mel_spectrogram.transpose(0, 1)\n",
    "\n",
    "        # eka bugi löyty (väärä shape (1))\n",
    "        max_input_length = max(max_input_length, mel_spectrogram.shape[0])\n",
    "\n",
    "        sentence = torch.tensor(alphabet.text_to_array(str.lower(item['sentence'])))\n",
    "\n",
    "        max_label_length = max(max_label_length, len(sentence))\n",
    "\n",
    "        audio.append(mel_spectrogram)\n",
    "        label.append(sentence)\n",
    "        audio_length.append(mel_spectrogram.shape[0])\n",
    "        label_length.append(len(sentence))\n",
    "\n",
    "    audio_padded = map(lambda x: torch.nn.functional.pad(x, (0, 0, 0, max_input_length - x.size(0))), audio)\n",
    "    labels_padded = map(lambda x: torch.nn.functional.pad(x, (0, max_label_length - len(x)), value=-1), label)\n",
    "\n",
    "    return (list(audio_padded), list(labels_padded), audio_length, label_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, label, audio_length, label_length = preprocess_audio(batch)\n",
    "print(len(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio[0].shape, audio[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label[1]), len(label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMCTC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers, dropout_rate):\n",
    "        super(LSTMCTC, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)  \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(ds['train'], batch_size=32, shuffle=True, num_workers=1, collate_fn=preprocess_audio)\n",
    "val_loader = DataLoader(ds['validation'], batch_size=32, shuffle=False, num_workers=1, collate_fn=preprocess_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init model\n",
    "device = torch.device('cuda')  \n",
    "\n",
    "model = LSTMCTC(\n",
    "        input_size=40,  # mels\n",
    "        hidden_size=320,\n",
    "        num_layers=4,\n",
    "        num_classes=len(alphabet.alphabet),\n",
    "        dropout_rate=0.2\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (audio, labels, audio_lengths, label_lengths) in enumerate(train_loader):\n",
    "        # Shape: (batch_size, max_input_length, num_mel_features)\n",
    "        audio = torch.stack(audio, dim=0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(audio)\n",
    "        # Shape: (max_input_length, batch_size, num_classes)\n",
    "        log_probs = F.log_softmax(outputs, dim=2).permute(1, 0, 2)\n",
    "\n",
    "        # Shape: (batch_size, max_label_length)\n",
    "        labels = torch.stack(labels, dim=0).to(device)\n",
    "        input_lengths = torch.tensor(audio_length).to(device)\n",
    "        target_lengths = torch.tensor(label_length).to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(log_probs, labels, audio_lengths, label_lengths)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/150], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
