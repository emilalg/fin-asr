{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from pywer import wer, cer\n",
    "import time\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMCTC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers, dropout_rate):\n",
    "        super(LSTMCTC, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)  \n",
    "        return self.fc(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "class Alphabet:\n",
    "    def __init__(self, alphabet_string = 'abcdefghijklmnopqrstuvwxyzåäö '):\n",
    "        self.alphabet = list(alphabet_string)\n",
    "        self.alphabet.append('<blank>')  # Adding blank token\n",
    "        self.char_to_index = {char: index for index, char in enumerate(self.alphabet)}\n",
    "        self.index_to_char = {index: char for index, char in enumerate(self.alphabet)}\n",
    "        self.blank_index = len(self.alphabet) - 1  # Blank is the last index\n",
    "\n",
    "    def text_to_array(self, text):\n",
    "        return [self.char_to_index[char] for char in str.lower(text) if char in self.char_to_index]\n",
    "\n",
    "    def array_to_text(self, array):\n",
    "        return ''.join(self.index_to_char.get(int(index), '<UNK>') for index in array)\n",
    "\n",
    "    def decode(self, log_probs: torch.Tensor, remove_blanks: bool = False) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode log probabilities to text using simple greedy decoding.\n",
    "        \n",
    "        Args:\n",
    "        log_probs (torch.Tensor): Log probabilities from the model\n",
    "                                  Shape: (batch_size, sequence_length, num_classes)\n",
    "        remove_blanks (bool): If True, remove all blank tokens from the output\n",
    "        \n",
    "        Returns:\n",
    "        List[str]: Decoded texts for each item in the batch\n",
    "        \"\"\"\n",
    "        # Get the most likely class at each step\n",
    "        predictions = torch.argmax(log_probs, dim=-1)  # Shape: (batch_size, sequence_length)\n",
    "        \n",
    "        batch_texts = []\n",
    "        for batch_item in predictions:\n",
    "            text = self._decode_prediction(batch_item, remove_blanks)\n",
    "            batch_texts.append(text)\n",
    "        \n",
    "        return batch_texts\n",
    "\n",
    "    def _decode_prediction(self, prediction: torch.Tensor, remove_blanks: bool) -> str:\n",
    "        \"\"\"\n",
    "        Decode a single prediction sequence to text.\n",
    "        \n",
    "        Args:\n",
    "        prediction (torch.Tensor): Prediction sequence for a single item\n",
    "                                   Shape: (sequence_length,)\n",
    "        remove_blanks (bool): If True, remove all blank tokens from the output\n",
    "        \n",
    "        Returns:\n",
    "        str: Decoded text\n",
    "        \"\"\"\n",
    "        decoded = []\n",
    "        previous = None\n",
    "        for p in prediction:\n",
    "            p = p.item()\n",
    "            if remove_blanks:\n",
    "                if p != self.blank_index and p != previous:\n",
    "                    if p < len(self.alphabet) - 1:  # Exclude blank token\n",
    "                        decoded.append(self.alphabet[p])\n",
    "            else:\n",
    "                if p != previous:\n",
    "                    if p < len(self.alphabet):  # Include blank token\n",
    "                        decoded.append(self.alphabet[p])\n",
    "            previous = p\n",
    "        \n",
    "        return ''.join(decoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "mel_spectrogram_converter = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_fft=400,\n",
    "        n_mels=40\n",
    ")\n",
    "\n",
    "alphabet = Alphabet()\n",
    "\n",
    "# from audio_utils\n",
    "def preprocess_audio(batch):\n",
    "\n",
    "    max_input_length = 0\n",
    "    max_label_length = 0\n",
    "\n",
    "    audio = []\n",
    "    label = []\n",
    "    audio_length = []\n",
    "    label_length = []\n",
    "\n",
    "    for item in batch:\n",
    "        audio_array = torch.tensor(item['audio']['array']).float()\n",
    "\n",
    "        sample_rate = item['audio']['sampling_rate']\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            audio_array = resampler(audio_array)\n",
    "        \n",
    "        audio_array = torchaudio.functional.preemphasis(audio_array)\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram_converter(audio_array)\n",
    "\n",
    "        # Apply log to the mel spectrogram\n",
    "        mel_spectrogram = torch.log(mel_spectrogram + 1e-9)\n",
    "\n",
    "        # Normalize the spectrogram\n",
    "        mel_spectrogram = (mel_spectrogram - mel_spectrogram.mean()) / mel_spectrogram.std()\n",
    "\n",
    "        # Transpose the mel spectrogram to correct dimension (time, mels)\n",
    "        mel_spectrogram = mel_spectrogram.transpose(0, 1)\n",
    "\n",
    "        # eka bugi löyty (väärä shape (1))\n",
    "        max_input_length = max(max_input_length, mel_spectrogram.shape[0])\n",
    "\n",
    "        sentence = torch.tensor(alphabet.text_to_array(str.lower(item['sentence'])))\n",
    "\n",
    "        max_label_length = max(max_label_length, len(sentence))\n",
    "\n",
    "        audio.append(mel_spectrogram)\n",
    "        label.append(sentence)\n",
    "        audio_length.append(mel_spectrogram.shape[0])\n",
    "        label_length.append(len(sentence))\n",
    "\n",
    "    audio_padded = map(lambda x: torch.nn.functional.pad(x, (0, 0, 0, max_input_length - x.size(0))), audio)\n",
    "\n",
    "    blank_index = len(alphabet.alphabet) - 1  # Index of the blank token\n",
    "    labels_padded = map(lambda x: torch.nn.functional.pad(x, (0, max_label_length - len(x)), value=blank_index), label)\n",
    "\n",
    "    return (list(audio_padded), list(labels_padded), audio_length, label_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\emila\\\\Documents\\\\koulu\\\\opinnaytetyo\\\\project\\\\notebooks'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_from_disk(dataset_path='../data/hf')\n",
    "val_dataset = dataset['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_model(model_path, device):\n",
    "    alphabet = Alphabet()\n",
    "    model = LSTMCTC(\n",
    "        input_size=40,\n",
    "        hidden_size=320,\n",
    "        num_layers=4,\n",
    "        num_classes=len(alphabet.alphabet),\n",
    "        dropout_rate=0.2\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model, alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(model_name, device):\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emila\\AppData\\Local\\Temp\\ipykernel_26524\\1431268298.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "custom_model_path = \"../gd.pth\"  # Adjust this path as needed\n",
    "custom_model, custom_alphabet = load_custom_model(custom_model_path, device)\n",
    "\n",
    "hf_model_name = \"Finnish-NLP/whisper-large-finnish-v3\"\n",
    "hf_model, hf_processor = load_hf_model(hf_model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hirvitys hyökkäsi sitä lähinnä olevan tankin kimppuun. torch.Size([1, 302400])\n"
     ]
    }
   ],
   "source": [
    "sample = val_dataset[0]\n",
    "sample_audio = torch.tensor(sample['audio']['array']).float().unsqueeze(0)\n",
    "sample_text = sample['sentence']\n",
    "\n",
    "print(f\"Original text: {sample_text}\", sample_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hirvitys hyökkäsi sitä lähinnä olevan tankin kimppuun.\n",
      "Äänestämme siis tämänsuuntaisten tarkistusten puolesta.\n",
      "Kuin kimppuun ampuen.\n",
      "Hän oli aina ollut niin taiteellinen, että arvasinkin, että hän voisi voittaa.\n",
      "Avasin silmäni, ja aloin tokkurassa prosessoida sitä, mitä Tukuk oli juuri sanonut.\n",
      "Ja siihen luottivat toisetkin.\n",
      "Äänestimme tietenkin tätä kohtaa vastaan.\n",
      "Äänestimme näin ollen nykyistä suositusta vastaan.\n",
      "Ja sillä oli korvat päässä, punaiset.\n",
      "Eikä rakkineessa ollut avaimellekaan paikkaa.\n",
      "Äänestin direktiivin hylkäämistä koskevan tarkistuksen puolesta.\n",
      "Äänestimme siis kyseisiä tarkistuksia vastaan ja koko tekstistä tyhjää.\n",
      "Polutonta rinnettä, sakeaan viidakkoon.\n",
      "Ylpeä se oli ollut kesällä.\n",
      "Äänestän tätä tekstiä vastaan.\n",
      "Äänestän näin ollen tyhjää, koska ehdotukseen sisältyy sekä kielteisiä että myönteisiä kohtia.\n",
      "Lääkäri saapui paikalle.\n",
      "Tuodakseen joukkoja maasta auttamaan vallankumouksessa Mysteerimies oli järjestänyt kymmenkunta sota-alusta maahan.\n",
      "Kuljettaja löysi paikalle ilman vaikeuksia\n",
      "Äänestimme siksi mietinnön puolesta.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(val_dataset[i]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model output: hirvitys hyöppäsisitä lähiillä olevan tankinki ppuun\n"
     ]
    }
   ],
   "source": [
    "processed_sample = preprocess_audio([sample])[0][0].unsqueeze(0).to(device)\n",
    "\n",
    "# Evaluate custom model\n",
    "with torch.no_grad():\n",
    "    custom_output = custom_model(processed_sample)\n",
    "    custom_log_probs = F.log_softmax(custom_output, dim=-1)\n",
    "    custom_decoded = custom_alphabet.decode(custom_log_probs, remove_blanks=True)[0]\n",
    "\n",
    "print(f\"Custom model output: {custom_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model output: Hallituksen suurta astetta häntä ylövyn pönkkäämisen mukaan.\n"
     ]
    }
   ],
   "source": [
    "hf_input = hf_processor(sample_audio.numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "# Evaluate Hugging Face model\n",
    "with torch.no_grad():\n",
    "    generated_ids = hf_model.generate(input_features=hf_input, language=\"fi\", task=\"transcribe\")\n",
    "    hf_decoded = hf_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Hugging Face model output: {hf_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model output: Hallituksen suurta astetta häntä ylövyn pönkkäämisen mukaan., gd Hirvitys hyökkäsi sitä lähinnä olevan tankin kimppuun.\n",
      "Hugging Face model output: “Äästä, äsästä, vääsyy, pörsää, tykö, assusta, tyy, ässtä...”, gd Äänestämme siis tämänsuuntaisten tarkistusten puolesta.\n",
      "Hugging Face model output: Venäjä, gd Kuin kimppuun ampuen.\n",
      "Hugging Face model output: Halluakki rääkkii yhteen takia ja työsäätiä päinvieskuja pois., gd Hän oli aina ollut niin taiteellinen, että arvasinkin, että hän voisi voittaa.\n",
      "Hugging Face model output: Yhdessä saa olla yhdenkytty yksi toisen sektorin yhdenkyttyä osuuksia., gd Avasin silmäni, ja aloin tokkurassa prosessoida sitä, mitä Tukuk oli juuri sanonut.\n",
      "Hugging Face model output: Prosessi on yksi peruste., gd Ja siihen luottivat toisetkin.\n",
      "Hugging Face model output: Rakennus on vahvistunut ja säästöt ovat vahvistuneet., gd Äänestimme tietenkin tätä kohtaa vastaan.\n",
      "Hugging Face model output: Käärästä huippua, säätää silmää., gd Äänestimme näin ollen nykyistä suositusta vastaan.\n",
      "Hugging Face model output: Vasaa, katoin poissa. Hyvä., gd Ja sillä oli korvat päässä, punaiset.\n",
      "Hugging Face model output: Aktovirta on syntynyt tähän tunturin puoleen., gd Eikä rakkineessa ollut avaimellekaan paikkaa.\n",
      "Hugging Face model output: Arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, että arvoisa Pappi, minä haluan sanoa, minä haluan sanoa, että arvoisa P, gd Äänestin direktiivin hylkäämistä koskevan tarkistuksen puolesta.\n",
      "Hugging Face model output: Raskeleet rahaa todella hankintaan ryhtyy ympäristötään., gd Äänestimme siis kyseisiä tarkistuksia vastaan ja koko tekstistä tyhjää.\n",
      "Hugging Face model output: Ymyysketaan laittoi  sötää taakasjulk., gd Polutonta rinnettä, sakeaan viidakkoon.\n",
      "Hugging Face model output: Ää, pääsen pääseen..., gd Ylpeä se oli ollut kesällä.\n",
      "Hugging Face model output: Rönnös on pötätäksäysty., gd Äänestän tätä tekstiä vastaan.\n",
      "Hugging Face model output: Rikostin saa olla kauan ajan, jos todellisuuden syntyessä on kaukana syö, että työpäässä yrittää., gd Äänestän näin ollen tyhjää, koska ehdotukseen sisältyy sekä kielteisiä että myönteisiä kohtia.\n",
      "Hugging Face model output: Röhötäräsyyt ja tuotannot., gd Lääkäri saapui paikalle.\n",
      "Hugging Face model output: Kybert sääti tänne yhdessä työnnetyksi, josta hän asioitaan alla yhdessä työnnetyksi., gd Tuodakseen joukkoja maasta auttamaan vallankumouksessa Mysteerimies oli järjestänyt kymmenkunta sota-alusta maahan.\n",
      "Hugging Face model output: Työterveys ja työterveysyhteistyö., gd Kuljettaja löysi paikalle ilman vaikeuksia\n",
      "Hugging Face model output: Katastolle säksä, matka on tyhjästä., gd Äänestimme siksi mietinnön puolesta.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    gd = val_dataset[i]['sentence']\n",
    "    audio = torch.tensor(val_dataset[i]['audio']['array']).float().unsqueeze(0)\n",
    "\n",
    "    hf_input = hf_processor(audio.numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    # Evaluate Hugging Face model\n",
    "    with torch.no_grad():   \n",
    "        generated_ids = hf_model.generate(input_features=hf_input, language=\"fi\", task=\"transcribe\")\n",
    "        hf_decoded = hf_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"Hugging Face model output: {hf_decoded}, gd {gd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Original audio shape: torch.Size([302400]), Sampling rate: 48000\n",
      "Resampled audio shape: torch.Size([100800]), New sampling rate: 16000\n",
      "Audio stats - Min: -0.9065377116203308, Max: 0.9499269723892212, Mean: -0.00010964000830426812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_scores` is. When `return_dict_in_generate` is not `True`, `output_scores` is ignored.\n",
      "  warnings.warn(\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Hirvitys hyökkäsi sitä lähinnä olevan tankin kimppuun.\n",
      "HF Model Output: Hirvitys hyökkäsi sitä lähinnä olevan tankin kimppuun.\n",
      "Position 1 top tokens: H, Hy, Hi,  Hir, Her\n",
      "\n",
      "Sample 2:\n",
      "Original audio shape: torch.Size([229824]), Sampling rate: 48000\n",
      "Resampled audio shape: torch.Size([76608]), New sampling rate: 16000\n",
      "Audio stats - Min: -0.8869452476501465, Max: 1.007727026939392, Mean: -3.974119681515731e-05\n",
      "Ground Truth: Äänestämme siis tämänsuuntaisten tarkistusten puolesta.\n",
      "HF Model Output: Äänestämme siis tämän suuntaisten tarkistusten puolesta.\n",
      "Position 1 top tokens: �, A, Ö,  ä,  Ä\n",
      "\n",
      "Sample 3:\n",
      "Original audio shape: torch.Size([148608]), Sampling rate: 48000\n",
      "Resampled audio shape: torch.Size([49536]), New sampling rate: 16000\n",
      "Audio stats - Min: -0.7911140322685242, Max: 0.9635905027389526, Mean: -9.09962909645401e-05\n",
      "Ground Truth: Kuin kimppuun ampuen.\n",
      "HF Model Output: Kuin kimppuun ampuen.\n",
      "Position 1 top tokens: K, ku, Qu, C,  K\n",
      "\n",
      "Sample 4:\n",
      "Original audio shape: torch.Size([319680]), Sampling rate: 48000\n",
      "Resampled audio shape: torch.Size([106560]), New sampling rate: 16000\n",
      "Audio stats - Min: -0.6401461958885193, Max: 0.9137948155403137, Mean: 9.41599046200281e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m hf_input \u001b[38;5;241m=\u001b[39m hf_processor(audio, sampling_rate\u001b[38;5;241m=\u001b[39msr, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 28\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m225\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m hf_decoded \u001b[38;5;241m=\u001b[39m hf_processor\u001b[38;5;241m.\u001b[39mbatch_decode(output\u001b[38;5;241m.\u001b[39msequences, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:671\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[0;32m    665\u001b[0m (\n\u001b[0;32m    666\u001b[0m     seek_sequences,\n\u001b[0;32m    667\u001b[0m     seek_outputs,\n\u001b[0;32m    668\u001b[0m     should_skip,\n\u001b[0;32m    669\u001b[0m     do_condition_on_prev_tokens,\n\u001b[0;32m    670\u001b[0m     model_output_type,\n\u001b[1;32m--> 671\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:834\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[1;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    831\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    832\u001b[0m         )\n\u001b[1;32m--> 834\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:2078\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2070\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2071\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2072\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2073\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2074\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2075\u001b[0m     )\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2078\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2084\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2085\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2089\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2090\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2091\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2092\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2098\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2099\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:3267\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3262\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m   3263\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m   3264\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3265\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 3267\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3268\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(\n\u001b[0;32m   3269\u001b[0m     next_token_scores_processed\n\u001b[0;32m   3270\u001b[0m )\n\u001b[0;32m   3272\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\generation\\logits_process.py:104\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\generation\\logits_process.py:1869\u001b[0m, in \u001b[0;36mSuppressTokensLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m   1868\u001b[0m     vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1869\u001b[0m     suppress_token_mask \u001b[38;5;241m=\u001b[39m \u001b[43misin_mps_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1870\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(suppress_token_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), scores)\n\u001b[0;32m   1871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\emila\\Documents\\koulu\\opinnaytetyo\\project\\env\\Lib\\site-packages\\transformers\\pytorch_utils.py:328\u001b[0m, in \u001b[0;36misin_mps_friendly\u001b[1;34m(elements, test_elements)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mtile(test_elements\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(test_elements\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(5):  # Test with 5 samples\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    hf_model_name = \"Finnish-NLP/whisper-large-finnish-v3\"\n",
    "    hf_processor = AutoProcessor.from_pretrained(hf_model_name)\n",
    "    hf_model = AutoModelForSpeechSeq2Seq.from_pretrained(hf_model_name).to(device)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    gd = val_dataset[i]['sentence']\n",
    "    audio = torch.tensor(val_dataset[i]['audio']['array']).float()\n",
    "    sr = val_dataset[i]['audio']['sampling_rate']\n",
    "    \n",
    "    print(f\"Original audio shape: {audio.shape}, Sampling rate: {sr}\")\n",
    "    \n",
    "    # Resample audio to 16000 Hz\n",
    "    if sr != 16000:\n",
    "        audio = resampler(audio)\n",
    "        sr = 16000\n",
    "    \n",
    "    print(f\"Resampled audio shape: {audio.shape}, New sampling rate: {sr}\")\n",
    "    print(f\"Audio stats - Min: {torch.min(audio)}, Max: {torch.max(audio)}, Mean: {torch.mean(audio)}\")\n",
    "    \n",
    "    hf_input = hf_processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = hf_model.generate(\n",
    "            input_features=hf_input,\n",
    "            language=\"fi\",\n",
    "            task=\"transcribe\",\n",
    "            max_length=225,\n",
    "            num_beams=5,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        \n",
    "    hf_decoded = hf_processor.batch_decode(output.sequences, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"Ground Truth: {gd}\")\n",
    "    print(f\"HF Model Output: {hf_decoded}\")\n",
    "    \n",
    "    # Print top 5 most likely tokens for the first 5 positions\n",
    "    token_probs = torch.softmax(output.scores[0], dim=-1)\n",
    "    top_tokens = torch.topk(token_probs[:5], k=5, dim=-1)\n",
    "    for pos, (values, indices) in enumerate(zip(top_tokens.values, top_tokens.indices)):\n",
    "        top_5 = [hf_processor.decode([idx.item()]) for idx in indices]\n",
    "        print(f\"Position {pos+1} top tokens: {', '.join(top_5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
